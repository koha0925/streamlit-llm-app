import streamlit as st
import os
from dotenv import load_dotenv
from openai import OpenAI # OpenAIã®æ–°ã—ã„Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
# ã“ã‚Œã«ã‚ˆã‚Šã€OPENAI_API_KEYãŒos.environã‹ã‚‰åˆ©ç”¨å¯èƒ½ã«ãªã‚‹
load_dotenv()

# OpenAI APIã‚­ãƒ¼ã®å–å¾—
# ãƒ­ãƒ¼ã‚«ãƒ«ã§ã¯.envã‹ã‚‰ã€Streamlit Cloudã§ã¯Secretsã‹ã‚‰èª­ã¿è¾¼ã¾ã‚Œã‚‹
api_key = os.getenv("OPENAI_API_KEY")

# APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®è­¦å‘Š
if not api_key:
    st.error("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯Streamlit Cloudã®Secretsã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop() # APIã‚­ãƒ¼ãŒãªã„å ´åˆã¯ã‚¢ãƒ—ãƒªã®å®Ÿè¡Œã‚’åœæ­¢

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
client = OpenAI(api_key=api_key)

# --- Webã‚¢ãƒ—ãƒªã®æ¦‚è¦ã¨æ“ä½œæ–¹æ³• ---
st.set_page_config(page_title="å°‚é–€å®¶ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒª", layout="centered") # ãƒšãƒ¼ã‚¸è¨­å®š

st.title("ğŸ¤– å°‚é–€å®¶ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒª")
st.markdown("""
ã“ã®ã‚¢ãƒ—ãƒªã¯ã€ã‚ãªãŸã®è³ªå•ã«å¯¾ã—ã¦ã€é¸æŠã—ãŸå°‚é–€å®¶ã®è¦–ç‚¹ã‹ã‚‰LLMï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ãŒå›ç­”ã—ã¾ã™ã€‚
ä»¥ä¸‹ã®æ‰‹é †ã§ã”åˆ©ç”¨ãã ã•ã„ï¼š

1.  **å°‚é–€å®¶ã‚’é¸æŠ:** ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã‹ã‚‰ã€å›ç­”ã—ã¦ã»ã—ã„å°‚é–€å®¶ã®ç¨®é¡ã‚’é¸ã³ã¾ã™ã€‚
2.  **è³ªå•ã‚’å…¥åŠ›:** ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã«è³ªå•ã‚’å…¥åŠ›ã—ã¾ã™ã€‚
3.  **é€ä¿¡:** ã€Œå›ç­”ã‚’ç”Ÿæˆã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€LLMãŒå›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

æ§˜ã€…ãªå°‚é–€å®¶ã«ãªã‚Šãã£ãŸLLMã®å›ç­”ã‚’ãŠæ¥½ã—ã¿ãã ã•ã„ï¼
""")

# --- å°‚é–€å®¶ã®ç¨®é¡ã¨ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å®šç¾© ---
# ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§é¸æŠã•ã‚Œã‚‹å°‚é–€å®¶ã¨ã€ãã‚Œã«å¯¾å¿œã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®šç¾©ã—ã¾ã™ã€‚
# ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ã€LLMã«ãã®å½¹å‰²ã‚’æŒ‡ç¤ºã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
expert_roles = {
    "ITã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ": "ã‚ãªãŸã¯ITã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚æŠ€è¡“çš„ãªèª²é¡Œè§£æ±ºã€ã‚·ã‚¹ãƒ†ãƒ å°å…¥ã€DXæ¨é€²ã«é–¢ã™ã‚‹å°‚é–€çŸ¥è­˜ã‚’æŒã¡ã€è«–ç†çš„ã‹ã¤å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚",
    "æ–™ç†ç ”ç©¶å®¶": "ã‚ãªãŸã¯æ–™ç†ç ”ç©¶å®¶ã§ã™ã€‚é£Ÿæã®çŸ¥è­˜ã€èª¿ç†æ³•ã€æ „é¤Šãƒãƒ©ãƒ³ã‚¹ã€é£Ÿæ–‡åŒ–ã«ç²¾é€šã—ã¦ãŠã‚Šã€ç¾å‘³ã—ãã¦å¥åº·çš„ãªãƒ¬ã‚·ãƒ”ã‚„é£Ÿã«é–¢ã™ã‚‹æƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚",
    "æ­´å²å­¦è€…": "ã‚ãªãŸã¯æ­´å²å­¦è€…ã§ã™ã€‚ä¸–ç•Œå²ã€æ—¥æœ¬å²ã€æ–‡åŒ–å²ãªã©å¹…åºƒã„æ­´å²çš„çŸ¥è­˜ã‚’æŒã¡ã€å®¢è¦³çš„ãªäº‹å®Ÿã«åŸºã¥ã„ãŸæ·±ã„æ´å¯Ÿã¨è§£èª¬ã‚’æä¾›ã—ã¾ã™ã€‚",
    "ã‚­ãƒ£ãƒªã‚¢ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼": "ã‚ãªãŸã¯ã‚­ãƒ£ãƒªã‚¢ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚å€‹äººã®ã‚¹ã‚­ãƒ«ã€çµŒé¨“ã€èˆˆå‘³ã‚’è€ƒæ…®ã—ã€è»¢è·ã€ã‚­ãƒ£ãƒªã‚¢ã‚¢ãƒƒãƒ—ã€è‡ªå·±æˆé•·ã«é–¢ã™ã‚‹å…·ä½“çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã¨ã‚µãƒãƒ¼ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚",
}

# --- LLMã¨ã®ã‚„ã‚Šå–ã‚Šã‚’è¡Œã†é–¢æ•° ---
# å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¨é¸æŠã•ã‚ŒãŸå°‚é–€å®¶ï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰ã‚’å—ã‘å–ã‚Šã€LLMã®å›ç­”ã‚’è¿”ã™
def get_llm_response(user_input: str, expert_system_message: str) -> str:
    """
    LLMï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¸¡ã—ã€å›ç­”ã‚’å–å¾—ã™ã‚‹é–¢æ•°ã€‚

    Args:
        user_input (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆï¼ˆè³ªå•ï¼‰ã€‚
        expert_system_message (str): LLMã«æŒ¯ã‚‹èˆã‚ã›ã‚‹å°‚é–€å®¶ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‚

    Returns:
        str: LLMã‹ã‚‰ã®å›ç­”ãƒ†ã‚­ã‚¹ãƒˆã€‚
    """
    try:
        # ChatCompletion APIã‚’å‘¼ã³å‡ºã™
        # messagesãƒªã‚¹ãƒˆã§ä¼šè©±ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å®šç¾©
        response = client.chat.completions.create(
            model="gpt-3.5-turbo", # ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š
            messages=[
                {"role": "system", "content": expert_system_message}, # å°‚é–€å®¶ã®å½¹å‰²ã‚’è¨­å®š
                {"role": "user", "content": user_input} # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            ],
            temperature=0.7, # å›ç­”ã®å‰µé€ æ€§ï¼ˆ0.0-1.0ã€é«˜ã„ã»ã©å‰µé€ çš„ï¼‰
            max_tokens=500,  # ç”Ÿæˆã•ã‚Œã‚‹å›ç­”ã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        )
        # å›ç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦è¿”ã™
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"LLMã‹ã‚‰ã®å›ç­”å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return "å›ç­”ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"

# --- Streamlit UIã®æ§‹ç¯‰ ---

# ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§å°‚é–€å®¶ã‚’é¸æŠ
selected_expert = st.radio(
    "å›ç­”ã—ã¦ã»ã—ã„å°‚é–€å®¶ã‚’é¸æŠã—ã¦ãã ã•ã„:",
    list(expert_roles.keys()), # expert_rolesã®ã‚­ãƒ¼ã‚’ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã®é¸æŠè‚¢ã«ã™ã‚‹
    index=0 # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ€åˆã®å°‚é–€å®¶ã‚’é¸æŠ
)

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
user_question = st.text_area("ã“ã“ã«è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", height=150)

# å›ç­”ç”Ÿæˆãƒœã‚¿ãƒ³
if st.button("å›ç­”ã‚’ç”Ÿæˆ"):
    if user_question:
        with st.spinner("å°‚é–€å®¶ãŒå›ç­”ã‚’è€ƒãˆã¦ã„ã¾ã™..."): # å‡¦ç†ä¸­ã«ã‚¹ãƒ”ãƒŠãƒ¼ã‚’è¡¨ç¤º
            # é¸æŠã•ã‚ŒãŸå°‚é–€å®¶ã«å¿œã˜ãŸã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
            system_message_for_llm = expert_roles[selected_expert]
            
            # LLMã¨ã®ã‚„ã‚Šå–ã‚Šé–¢æ•°ã‚’å‘¼ã³å‡ºã™
            llm_answer = get_llm_response(user_question, system_message_for_llm)
            
            # å›ç­”ã‚’ç”»é¢ã«è¡¨ç¤º
            st.subheader(f"âœ¨ {selected_expert}ã‹ã‚‰ã®å›ç­”:")
            st.write(llm_answer)
    else:
        st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

